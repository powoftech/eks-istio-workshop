var relearn_searchindex = [
  {
    "breadcrumb": "",
    "content": "Workshop description This comprehensive AWS workshop teaches you to build, secure, and monitor containerized applications using modern cloud-native technologies and DevSecOps practices. You’ll create a complete end-to-end secure container pipeline that encompasses vulnerability scanning, policy enforcement, and runtime threat detection.\nThroughout this hands-on workshop, you’ll:\nBuild a secure CI/CD pipeline using GitHub Actions that automatically builds, scans, and deploys containerized applications Implement container vulnerability scanning with Amazon ECR’s integrated security features Deploy policy enforcement using Kyverno as a Kubernetes admission controller to prevent insecure workloads Set up runtime threat detection with Falco to monitor and alert on suspicious container behavior Provision and manage Amazon EKS clusters with security best practices Apply DevSecOps principles by integrating security into every stage of the development lifecycle By the end of this workshop, you’ll have a production-ready, security-first container deployment pipeline that automatically prevents vulnerable images from reaching production and detects threats in real-time.\nIntended audience This workshop is designed for:\nDevOps Engineers looking to implement security controls in their container pipelines Security Engineers wanting to learn cloud-native security tools and practices Platform Engineers building secure Kubernetes platforms for development teams Software Developers interested in understanding container security and secure deployment practices Cloud Architects designing secure containerized solutions on AWS Site Reliability Engineers (SREs) implementing security monitoring and policy enforcement Assumed knowledge Participants should have:\nBasic containerization experience with Docker (building images, running containers) Fundamental Kubernetes knowledge (pods, deployments, services, namespaces) AWS foundational understanding (basic familiarity with AWS services and concepts) Command-line proficiency in Linux/Unix environments Git and GitHub experience for version control and basic CI/CD concepts YAML syntax familiarity for Kubernetes manifests and configuration files Previous experience with EKS, security tools, or policy engines is helpful but not required as the workshop provides step-by-step guidance.\nTime to complete the workshop Total Duration: 3-4 hours\nModule Breakdown:\nSetup and Prerequisites: 30 minutes Project Repository Creation: 20 minutes Application and CI Workflow: 45 minutes EKS Cluster Provisioning: 30 minutes Kyverno Policy Engine Setup: 45 minutes Falco Runtime Security: 30 minutes Testing and Validation: 30 minutes Clean-up: 15 minutes The workshop is designed to be completed in a single session, with natural break points after each major module. All infrastructure provisioning and deployments are included in the timing estimates.",
    "description": "Workshop description This comprehensive AWS workshop teaches you to build, secure, and monitor containerized applications using modern cloud-native technologies and DevSecOps practices. You’ll create a complete end-to-end secure container pipeline that encompasses vulnerability scanning, policy enforcement, and runtime threat detection.\nThroughout this hands-on workshop, you’ll:\nBuild a secure CI/CD pipeline using GitHub Actions that automatically builds, scans, and deploys containerized applications Implement container vulnerability scanning with Amazon ECR’s integrated security features Deploy policy enforcement using Kyverno as a Kubernetes admission controller to prevent insecure workloads Set up runtime threat detection with Falco to monitor and alert on suspicious container behavior Provision and manage Amazon EKS clusters with security best practices Apply DevSecOps principles by integrating security into every stage of the development lifecycle By the end of this workshop, you’ll have a production-ready, security-first container deployment pipeline that automatically prevents vulnerable images from reaching production and detects threats in real-time.",
    "tags": [],
    "title": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "Welcome to this comprehensive AWS workshop where you’ll learn to build, secure, and monitor containerized applications using modern cloud-native technologies. This hands-on workshop will guide you through creating a complete DevSecOps pipeline for Kubernetes applications.\nAmazon Elastic Container Registry (ECR) Amazon Elastic Container Registry (ECR) is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with Amazon EKS and Amazon ECS, simplifying your development to production workflow.\nKey Features:\nFully Managed: No infrastructure to manage or maintain Secure: Images are encrypted at rest and in transit, with vulnerability scanning Highly Available: Built on Amazon S3 for 99.999999999% (11 9’s) durability Integrated: Works seamlessly with AWS services and IAM for fine-grained access control Cost-Effective: Pay only for the storage you use with no upfront fees In this workshop, you’ll use ECR to store your containerized application images and integrate it with your CI/CD pipeline.\nAmazon Elastic Kubernetes Service (EKS) Amazon Elastic Kubernetes Service (EKS) is a fully managed Kubernetes service that makes it easy to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes.\nKey Benefits:\nFully Managed Control Plane: AWS manages the Kubernetes control plane, including high availability and security patches Secure by Default: Integrated with AWS IAM, VPC, and security groups Highly Available: Control plane runs across multiple Availability Zones Kubernetes Compatibility: Regular updates to support the latest Kubernetes versions Integration: Works with AWS services like ECR, ALB, EBS, EFS, and CloudWatch In this workshop, you’ll provision an EKS cluster to deploy and manage your containerized applications with advanced security and monitoring capabilities.\nGitHub Actions GitHub Actions is a powerful CI/CD platform that allows you to automate your software development workflows directly from your GitHub repository. It enables you to build, test, and deploy your code right from GitHub.\nKey Capabilities:\nWorkflow Automation: Automate build, test, and deployment processes Event-Driven: Trigger workflows on GitHub events like push, pull request, or release Marketplace: Access thousands of pre-built actions from the community Matrix Builds: Test across multiple operating systems and versions simultaneously Secrets Management: Securely store and use sensitive information in workflows In this workshop, you’ll create GitHub Actions workflows to automatically build, scan, and deploy your applications to EKS.\nKyverno Policy Engine Kyverno is a policy engine designed for Kubernetes that allows you to manage cluster policies as code. It provides a declarative approach to policy management without requiring a new language.\nKey Features:\nYAML-Based Policies: Write policies using familiar Kubernetes YAML syntax Validation: Enforce rules for resource configurations Mutation: Automatically modify resources to comply with standards Generation: Create additional resources based on policy rules Reporting: Generate policy violation reports In this workshop, you’ll use Kyverno as a cluster gatekeeper to enforce security policies and governance rules.\nFalco Runtime Security Falco is an open-source runtime security tool that detects unexpected application behavior and alerts on threats at runtime. It acts as a security camera for your Kubernetes clusters.\nSecurity Capabilities:\nRuntime Threat Detection: Monitor kernel calls and detect suspicious activities Kubernetes-Aware: Understand Kubernetes contexts and resources Flexible Rules: Define custom rules for your specific security requirements Multiple Outputs: Send alerts to various destinations (Slack, PagerDuty, etc.) Cloud-Native: Designed specifically for containerized environments In this workshop, you’ll deploy Falco to monitor your applications and detect potential security threats in real-time.",
    "description": "Welcome to this comprehensive AWS workshop where you’ll learn to build, secure, and monitor containerized applications using modern cloud-native technologies. This hands-on workshop will guide you through creating a complete DevSecOps pipeline for Kubernetes applications.\nAmazon Elastic Container Registry (ECR) Amazon Elastic Container Registry (ECR) is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with Amazon EKS and Amazon ECS, simplifying your development to production workflow.",
    "tags": [],
    "title": "Introduction",
    "uri": "/introduction/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "Before starting this workshop, you’ll need to install and configure several essential tools that will help you interact with AWS services and Kubernetes clusters. These tools form the foundation for working with containers and AWS resources, especially Amazon EKS.\nAWS CLI The AWS Command Line Interface (CLI) is a unified tool that allows you to manage AWS services from your terminal. You’ll use it to configure your AWS credentials, create and manage AWS resources, and interact with various AWS services throughout this workshop.\nInstallation: Download and install the AWS CLI from the official AWS documentation.\nConfiguration: After installation, run aws configure to set up your access keys, default region, and output format.\nkubectl kubectl is the Kubernetes command-line tool that allows you to run commands against Kubernetes clusters. You’ll use it to deploy applications, inspect and manage cluster resources, and view logs in your EKS cluster.\nInstallation: Follow the Kubernetes documentation to install kubectl for your operating system.\nVerification: Run kubectl version --client to verify the installation.\neksctl eksctl is a command-line tool for creating and managing Kubernetes clusters on Amazon EKS. It simplifies the process of creating EKS clusters and worker nodes, handling much of the underlying AWS infrastructure setup automatically.\nInstallation: Download eksctl from the official GitHub releases or use your package manager.\nVerification: Run eksctl version to confirm the installation.\nHelm Helm is a package manager for Kubernetes that helps you manage Kubernetes applications. You’ll use Helm to install and configure Kyverno, Falo and other applications on your EKS cluster using pre-configured charts.\nInstallation: Install Helm following the official Helm documentation.\nVerification: Run helm version to verify the installation.\nDocker Docker is a containerization platform that allows you to build, package, and run applications in containers. You’ll use Docker to build container images for your applications and understand how containerized workloads operate in Kubernetes environments.\nInstallation: Download and install Docker Desktop from the official Docker website or use your system’s package manager for Docker Engine.\nVerification: Run docker --version to confirm the installation and docker run hello-world to test that Docker can pull and run containers.\nNote: Ensure all tools are added to your system’s PATH and are accessible from your terminal before proceeding.",
    "description": "Before starting this workshop, you’ll need to install and configure several essential tools that will help you interact with AWS services and Kubernetes clusters. These tools form the foundation for working with containers and AWS resources, especially Amazon EKS.\nAWS CLI The AWS Command Line Interface (CLI) is a unified tool that allows you to manage AWS services from your terminal. You’ll use it to configure your AWS credentials, create and manage AWS resources, and interact with various AWS services throughout this workshop.",
    "tags": [],
    "title": "Prerequisites",
    "uri": "/prerequisites/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "Every good project needs a home.\nGo to GitHub (or your preferred Git provider).\nCreate a new, public repository. Name it something like secure-container-pipeline.\nClone the repository to your local machine:\ngit clone \u003cyour-repository-url\u003e cd secure-container-pipeline",
    "description": "Every good project needs a home.\nGo to GitHub (or your preferred Git provider).\nCreate a new, public repository. Name it something like secure-container-pipeline.\nClone the repository to your local machine:\ngit clone \u003cyour-repository-url\u003e cd secure-container-pipeline",
    "tags": [],
    "title": "Create the Project Repository",
    "uri": "/create-project-repository/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "Create the Sample Application and Dockerfile First, we need something to build. We’ll create a very simple Node.js “Hello World” application and a security-conscious Dockerfile.\nIn your secure-container-pipeline project directory, create a new folder named app.\nmkdir app cd app Create the Node.js application file app.js:\n# In the 'app' directory touch app.js Paste this simple server code into app/app.js:\n// app/app.js const http = require(\"http\"); const port = 8080; const server = http.createServer((req, res) =\u003e { res.statusCode = 200; res.setHeader(\"Content-Type\", \"text/plain\"); res.end(\"Hello, FCJ-ers!\\n\"); }); server.listen(port, () =\u003e { console.log(`Server running on port ${port}`); }); Create the Dockerfile:\n# app/Dockerfile # Stage 1: Use a specific, slim base image to reduce attack surface. FROM node:22-slim AS base # Create a dedicated, non-root user and group for the application. # This is a critical security measure. RUN addgroup --system --gid 1001 nodejs RUN adduser --system --uid 1001 appuser WORKDIR /home/appuser/app # Copy only the necessary file and set correct permissions. COPY --chown=appuser:nodejs app.js . # Switch to the non-root user. Any subsequent commands run as this user. USER appuser # Expose the port the app runs on. EXPOSE 8080 # Command to run the application. CMD [ \"node\", \"app.js\" ] Go back to the root of your project directory:\ncd .. Create the ECR Repository Let’s create the secure container registry where we’ll store our Docker images.\nRun this AWS CLI command in your terminal:\naws ecr create-repository \\ --repository-name workshop-app \\ --image-scanning-configuration scanOnPush=true \\ --region us-east-2 # Use the same region as your cluster Info The --image-scanning-configuration scanOnPush=true flag is our first deliberate security control. We’ve instructed AWS to automatically scan every new image we push to this repository for known vulnerabilities (CVEs). This is a foundational piece of our secure pipeline.\nSet Up Secure Access from GitHub Actions to AWS (OIDC) We need to grant GitHub the permission to push images to our ECR repository. We will use the modern, secure, passwordless method: OIDC (OpenID Connect).\nIn the AWS Console, go to IAM → Identity providers. Click Add provider. Select OpenID Connect. For Provider URL, enter https://token.actions.githubusercontent.com. For Audience, enter sts.amazonaws.com. Click Add provider. Create the IAM Role for GitHub Actions. Go to IAM → Roles → Create role. For Trusted entity type, select Web identity. From the Identity provider dropdown, select the token.actions.githubusercontent.com provider you just created. For Audience, select sts.amazonaws.com. For GitHub organization/repository, enter your details. For a personal project, you can be specific: Organization: your-github-username Repository: secure-container-pipeline (Optional but recommended) Branch: main or master Click Next. On the Add permissions screen, find and attach the AmazonEC2ContainerRegistryPowerUser policy. This gives just enough permission to log in and push images to ECR. Click Next. Give the role a name, like WorkshopGitHubActionsRole (Remember the role name. You will use this role to deploy to the EKS cluster later) Create the role. CRITICAL: Click on the new role you just created and copy its ARN. It will look like arn:aws:iam::\u003c\u003cAWS Account ID\u003e\u003e:role/WorkshopGitHubActionsRole. You will need this for the next step. Create the GitHub Actions CI Workflow This is the heart of our automated build and scan process.\nCreate the workflow directory structure:\nmkdir -p .github/workflows Create the workflow file ci.yml:\ntouch .github/workflows/ci.yml Paste the following YAML into .github/workflows/ci.yml. Replace the placeholder with your actual Role ARN.\n# .github/workflows/ci.yml name: CI Workflow for EKS Workshop # This workflow runs on any push to the main branch on: push: branches: [main] # Allows you to run this workflow manually from the Actions tab workflow_dispatch: env: AWS_REGION: us-east-2 # Your AWS region ECR_REPOSITORY: workshop-app # Your ECR repository name EKS_CLUSTER_NAME: workshop-cluster # Your EKS cluster name jobs: build-scan-push: name: Build, Scan \u0026 Push runs-on: ubuntu-latest outputs: image: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ steps.image-def.outputs.tag }} permissions: # Required for OIDC connection to AWS id-token: write contents: read steps: - name: Checkout repository uses: actions/checkout@v5 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v4 with: role-to-assume: arn:aws:iam::\u003c\u003cAWS Account ID\u003e\u003e:role/WorkshopGitHubActionsRole # \u003c-- PASTE YOUR ROLE ARN HERE aws-region: ${{ env.AWS_REGION }} - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v2 - name: Define image tag id: image-def run: echo \"tag=${{ github.sha }}\" \u003e\u003e $GITHUB_OUTPUT - name: Build, tag, and push image to Amazon ECR id: build-image env: ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }} IMAGE_TAG: ${{ steps.image-def.outputs.tag }} run: | docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG -f app/Dockerfile ./app docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG - name: Security Scan with Trivy uses: aquasecurity/trivy-action@master with: image-ref: \"${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ steps.image-def.outputs.tag }}\" format: \"table\" # Fail the build if Trivy finds any HIGH or CRITICAL severity vulnerabilities exit-code: \"1\" ignore-unfixed: true vuln-type: \"os,library\" severity: \"CRITICAL,HIGH\" Commit and push to trigger the workflow\nAdd all your new files to Git, commit them, and push:\ngit add . git commit -m \"feat: Add sample app, Dockerfile, and initial CI workflow\" git push origin main Observe the magic! Go to your GitHub repository, click on the Actions tab. You will see your workflow running. Click on it to see the logs for each step. It will:\nCheck out the code. Securely connect to AWS. Log in to ECR. Build and push your Docker image. Crucially, it will then run Trivy to scan the image you just pushed.",
    "description": "Create the Sample Application and Dockerfile First, we need something to build. We’ll create a very simple Node.js “Hello World” application and a security-conscious Dockerfile.\nIn your secure-container-pipeline project directory, create a new folder named app.\nmkdir app cd app Create the Node.js application file app.js:\n# In the 'app' directory touch app.js Paste this simple server code into app/app.js:\n// app/app.js const http = require(\"http\"); const port = 8080; const server = http.createServer((req, res) =\u003e { res.statusCode = 200; res.setHeader(\"Content-Type\", \"text/plain\"); res.end(\"Hello, FCJ-ers!\\n\"); }); server.listen(port, () =\u003e { console.log(`Server running on port ${port}`); }); Create the Dockerfile:",
    "tags": [],
    "title": "Create a Secure, Buildable Application and a CI Workflow",
    "uri": "/create-secure-buildable-application-and-ci-workflow/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "Create the EKS Cluster Configuration We will use eksctl to build the cluster. The best practice is to define the cluster in a configuration file, which you can then commit to your Git repository for version control and reproducibility.\nIn the root of your project directory, create a new folder named k8s.\nmkdir k8s cd k8s Create a new file named cluster.yaml\nPaste the following content into cluster.yaml. Read the comments to understand what each line does\n# k8s/cluster.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: # The name of your cluster name: workshop-cluster # The AWS region where the cluster will be created region: us-east-2 # The Kubernetes version of your cluster version: \"1.33\" # This section defines the Kubernetes worker nodes nodeGroups: - name: ng-1-workers # Name for the node group instanceType: t3.medium # A default general-purpose instance type. desiredCapacity: 2 # Start with 2 nodes for high availability minSize: 1 # For cost savings, you can scale down to 1 node when not actively testing maxSize: 3 # Limit max size to prevent accidental cost overruns # Recommended: Use AWS's Bottlerocket OS for better security and smaller footprint amiFamily: Bottlerocket # Recommend: Launch nodegroup in private subnets privateNetworking: true accessConfig: authenticationMode: API_AND_CONFIG_MAP # Create an EKS access entry to help GitHub Actions workflow can deploy to the cluster accessEntries: # IMPORTANT: Repalce \u003c\u003cAWS Account ID\u003e\u003e with your AWS Account ID - principalARN: arn:aws:iam::\u003c\u003cAWS Account ID\u003e\u003e:role/WorkshopGitHubActionsRole accessPolicies: - policyARN: arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster Launch the EKS Cluster Now, execute the command to build the cluster.\nOpen your terminal in the secure-container-pipeline directory.\nRun the creation command:\neksctl create cluster -f k8s/cluster.yaml Be patient. This process will take 15-20 minutes. eksctl will print out a lot of information as it provisions the resources in AWS CloudFormation. Go grab a coffee.\nConfirmation: Once it’s finished, eksctl will automatically update your local kubeconfig file (~/.kube/config). This means kubectl will now point to your new EKS cluster.\nVerify Cluster Access Let’s make sure you can talk to your new cluster.\nRun this kubectl command:\nkubectl get nodes You should see an output listing your two worker nodes, similar to this:\nNAME STATUS ROLES AGE VERSION ip-192-168-158-45.us-east-2.compute.internal Ready \u003cnone\u003e 6m16s v1.33.1-eks-b9364f6 ip-192-168-173-179.us-east-2.compute.internal Ready \u003cnone\u003e 6m21s v1.33.1-eks-b9364f6 Create Kubernetes Deployment Manifests We need to tell Kubernetes how to run our application. We’ll do this with two standard Kubernetes resource files: a Deployment (to manage the application Pods) and a Service (to expose the application to traffic).\nIn the root of your project directory, create a new folder named k8s.\nmkdir k8s cd k8s Create the deployment.yaml file:\n# In the 'k8s' directory touch deployment.yaml Paste this content into k8s/deployment.yaml. Pay close attention to the ##IMAGE_TAG_PLACEHOLDER## line; we will replace this dynamically in our pipeline.\n# k8s/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: workshop-app labels: app: workshop-app spec: replicas: 2 # Run two instances for availability selector: matchLabels: app: workshop-app template: metadata: labels: app: workshop-app spec: containers: - name: workshop-app # IMPORTANT: This is a placeholder. Our pipeline will replace it. image: \"IMAGE_PLACEHOLDER\" ports: - containerPort: 8080 # --- Security Context --- # This enforces security best practices at the container level. securityContext: # Prevents the container from gaining more privileges than its parent process. allowPrivilegeEscalation: false # Runs the container with a read-only root filesystem. readOnlyRootFilesystem: true # Reinforcing our Dockerfile's non-root user. runAsNonRoot: true # Specifies the user and group IDs to run as, matching our Dockerfile. runAsUser: 1001 runAsGroup: 1001 # Drops all Linux capabilities and only adds back what's necessary (none in this case). capabilities: drop: - \"ALL\" # Location for temporary files, as the root filesystem is read-only. volumes: - name: tmp emptyDir: {} Create the service.yaml file:\n# In the 'k8s' directory touch service.yaml Paste this content into k8s/service.yaml. This will create a LoadBalancer service, which automatically provisions an AWS Network Load Balancer to expose your application to the internet.\n# k8s/service.yaml apiVersion: v1 kind: Service metadata: name: workshop-app-service spec: selector: app: workshop-app # This type creates an external AWS Load Balancer type: LoadBalancer ports: - protocol: TCP port: 80 # The port the load balancer listens on targetPort: 8080 # The port the container listens on Go back to the root of your project directory:\ncd .. Update the IAM Role for Deployment Permissions Our WorkshopGitHubActionsRole can push to ECR, but it can’t talk to our EKS cluster yet. We need to grant it permission.\nIn the AWS Console, go to IAM → Roles →.\nSelect WorkshopGitHubActionsRole role.\nIn Permissions tab, Permissions policies section, select Add permissions → Create inline policy.\nFor Policy editor, select JSON, paste the following policy and replace the AWS account ID:\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement1\", \"Effect\": \"Allow\", \"Action\": \"eks:DescribeCluster\", \"Resource\": \"arn:aws:eks:us-east-2:\u003c\u003cAWS Account ID\u003e\u003e:cluster/workshop-cluster\" } ] } Click Next.\nGive the policy a name, like DescribeWorkshopEKSCluster.\nCreate the policy.\nUpdate the GitHub Actions Workflow to Deploy Now, we’ll add a new deploy job to our ci.yml file.\nOpen .github/workflows/ci.yml.\nAdd the new deploy job to the end of the file. The complete, updated file should look like this:\n# .github/workflows/ci.yml # ... (other params omitted) ... jobs: build-scan-push: # ... (the build-scan-push job remains exactly the same as before) ... # ---- NEW DEPLOY JOB ---- deploy: # This job will only run if the 'build-scan-push' job succeeds name: Deploy to EKS needs: build-scan-push runs-on: ubuntu-latest permissions: # Required for OIDC connection to AWS id-token: write contents: read steps: - name: Checkout repository uses: actions/checkout@v5 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v4 with: role-to-assume: arn:aws:iam::\u003c\u003cAWS Account ID\u003e\u003e:role/WorkshopGitHubActionsRole # \u003c-- PASTE YOUR ROLE ARN HERE aws-region: ${{ env.AWS_REGION }} - name: Set up Kubeconfig for EKS run: aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} - name: Substitute image tag in Kubernetes manifest run: | sed -i 's|IMAGE_PLACEHOLDER|${{ needs.build-scan-push.outputs.image }}|' k8s/deployment.yaml - name: Deploy to EKS cluster run: | echo \"--- Applying deployment.yaml ---\" cat k8s/deployment.yaml kubectl apply -f k8s/deployment.yaml echo \"--- Applying service.yaml ---\" kubectl apply -f k8s/service.yaml Key Changes:\nA new deploy job is added. needs: build-scan-push ensures deployment only happens after a successful build and scan. The sed command is a crucial step that finds our IMAGE_PLACEHOLDER and replaces it with the actual, unique image URI from the build step. kubectl apply sends our configuration to the EKS cluster. Commit, push, and verify deployment:\nCommit your changes:\ngit add . git commit -m \"feat: Add k8s manifests and deploy job to CI workflow\" git push origin main Watch the pipeline: Go to the Actions tab in GitHub. You’ll see the full pipeline run. This time, after “Build, Scan \u0026 Push” completes, the “Deploy to EKS” job will start.\nVerify in your terminal: Once the pipeline succeeds, check the status of your deployment.\nCheck the pods:\nkubectl get pods -l app=workshop-app You should see two pods with a Running status.\nCheck the service and get the Load Balancer URL:\nkubectl get service workshop-app-service It will take a minute or two for AWS to provision the load balancer. The EXTERNAL-IP will change from \u003cpending\u003e to a long DNS name.\nTest the application! Copy the EXTERNAL-IP DNS name and paste it into your web browser. You should see the message: Hello, FCJ-ers!",
    "description": "Create the EKS Cluster Configuration We will use eksctl to build the cluster. The best practice is to define the cluster in a configuration file, which you can then commit to your Git repository for version control and reproducibility.\nIn the root of your project directory, create a new folder named k8s.\nmkdir k8s cd k8s Create a new file named cluster.yaml\nPaste the following content into cluster.yaml. Read the comments to understand what each line does",
    "tags": [],
    "title": "Provision the EKS Cluster and Automate Deployment",
    "uri": "/provision-eks-cluster-and-ecr-repository/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "Our goal here is to make the cluster self-defending. The pipeline we built is great, but it doesn’t stop someone with kubectl access from manually deploying an insecure container. We will now install a gatekeeper–an admission controller–to enforce our rules on every single workload that tries to run on the cluster.\nOur tool for this is Kyverno. It’s powerful, Kubernetes-native, and the policies are just simple YAML, making it perfect for our project.\nInstall Kyverno using Helm Helm is the best way to install and manage complex applications like Kyverno on Kubernetes.\nOpen your terminal, ensuring your kubectl context is pointing to your workshop-cluster.\nAdd the Kyverno Helm repository:\nhelm repo add kyverno https://kyverno.github.io/kyverno/ Update your Helm repositories to fetch the new chart information:\nhelm repo update Install Kyverno into its own dedicated namespace. This is a best practice for cluster-wide tools.\nhelm install kyverno kyverno/kyverno -n kyverno --create-namespace Verify the installation. It might take a minute for all the Kyverno pods to be ready.\nkubectl get pods -n kyverno You should see several pods, including the admission controller, background controller, and cleanup controller, all in a Running state.\nCreate Your First Security Policies Now for the powerful part. We’ll define our security rules as ClusterPolicy resources. These are cluster-wide rules.\nIn your project’s k8s directory, create a new file named policy-disallow-latest-tag.yaml:\n# In the k8s directory touch policy-disallow-latest-tag.yaml Paste the following policy. This is a fundamental best practice: it prevents ambiguous deployments and forces the use of specific, immutable tags (like the Git SHA we use in our pipeline).\n# k8s/policy-disallow-latest-tag.yaml apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: disallow-latest-tag spec: # This rule applies to all Pods, Deployments, StatefulSets, etc. validationFailureAction: Enforce background: true rules: - name: require-not-latest-tag match: any: - resources: kinds: - Pod validate: message: \"Using the 'latest' image tag is a security risk and is not allowed. Please use a specific image tag.\" pattern: spec: containers: # This pattern says \"the image field of ANY container MUST NOT end with :latest\" - image: \"!*:latest\" Info validationFailureAction: Enforce is the key. It tells Kyverno to block any API request that violates this rule. The alternative, Audit, would only log the violation. We want to be strict.\nCreate a second policy file named policy-require-non-root.yaml:\n# In the k8s directory touch policy-require-non-root.yaml Paste this policy. It enforces that containers cannot run as the root user, which drastically reduces the blast radius if a container is compromised. This turns the best practice from our Dockerfile into a non-negotiable cluster-wide rule.\n# k8s/policy-require-non-root.yaml apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: require-non-root-user spec: validationFailureAction: Enforce background: true rules: - name: check-for-non-root match: any: - resources: kinds: - Pod exclude: any: # Exclude Falco and Starboard namespace - resources: namespaces: - falco - falco-system - starboard - starboard-system - starboard-operator # Exclude pods with privileged system labels - resources: selector: matchLabels: app.kubernetes.io/name: falco # Exclude system namespaces that commonly need root - resources: namespaces: - kube-system - istio-system - monitoring validate: message: \"Containers must not run as root. Set spec.securityContext.runAsNonRoot to true at pod or container level.\" anyPattern: # Pattern 1: Pod-level security context with runAsNonRoot: true - spec: securityContext: runAsNonRoot: true # Pattern 2: All containers have runAsNonRoot: true in their security context - spec: containers: - securityContext: runAsNonRoot: true Apply and Test the Policies Apply the policies to your cluster using kubectl:\nkubectl apply -f k8s/policy-disallow-latest-tag.yaml kubectl apply -f k8s/policy-require-non-root.yaml Test the “disallow-latest-tag” policy. Now, try to manually create a pod using the latest tag. This simulates what a developer might do, bypassing your CI/CD pipeline.\nkubectl run test-pod --image=nginx:latest The request will be rejected! You will receive an error message directly from the Kubernetes API server, containing the custom message from your policy:\nError from server: admission webhook \"validate.kyverno.svc-fail\" denied the request: resource Pod/default/test-pod was blocked due to the following policies disallow-latest-tag: require-not-latest-tag: 'validation error: Using the ''latest'' image tag is a security risk and is not allowed. Please use a specific image tag. rule require-not-latest-tag failed at path /spec/containers/0/image/' Test the “require-non-root” policy. Now try to run a default container, which typically runs as root.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: test-root-pod namespace: default spec: containers: - name: nginx image: nginx:1.21 # This pod should be BLOCKED by the policy since it doesn't specify runAsNonRoot EOF This will also be rejected with a clear message explaining that containers must be configured to run as non-root.\nClean up the failed test pods (they won’t have been created, but kubectl run may create a deployment object):\nkubectl delete pod test-root-pod-blocked --ignore-not-found Commit your new policy files to your Git repository.\ngit add . git commit -m \"feat: Implement Kyverno admission control policies\" git push origin main",
    "description": "Our goal here is to make the cluster self-defending. The pipeline we built is great, but it doesn’t stop someone with kubectl access from manually deploying an insecure container. We will now install a gatekeeper–an admission controller–to enforce our rules on every single workload that tries to run on the cluster.\nOur tool for this is Kyverno. It’s powerful, Kubernetes-native, and the policies are just simple YAML, making it perfect for our project.",
    "tags": [],
    "title": "Install and Configure Kyverno as a Cluster Gatekeeper",
    "uri": "/install-and-configure-kyverno-as-cluster-gatekeeper/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "We will focus first on the runtime security piece. Our mission is to detect and alert on suspicious activity within our running containers in real-time.\nOur tool for this is Falco, the CNCF open-source standard for cloud-native runtime threat detection. It acts like a security camera that watches system calls and alerts you when a container does something it shouldn’t.\nInstall Falco using Helm Just like with Kyverno, Helm is the most straightforward way to deploy Falco and its components.\nOpen your terminal and ensure you’re connected to your EKS cluster.\nAdd the Falco Security Helm repository:\nhelm repo add falcosecurity https://falcosecurity.github.io/charts Update your Helm repositories:\nhelm repo update Install Falco into its own falco namespace. Falco works by deploying a DaemonSet, which means it will run one Falco pod on each of your worker nodes to monitor all activity on that node.\nhelm install --replace falco --namespace falco --create-namespace --set tty=true falcosecurity/falco Verify the installation. It may take a minute or two for the pods to start.\nkubectl get pods -n falco Observe Falco’s Logs Falco’s default behavior is to output its alerts to its logs. Let’s watch them to see what normal activity looks like and to prepare to see an alert.\nTail the logs from all Falco pods. The -f flag will “follow” the logs, streaming them to your terminal in real-time. kubectl logs -n falco -l app.kubernetes.io/name=falco -f Trigger a Security Alert We will now simulate a common attack pattern: an attacker gains access to a running container and tries to escalate privileges or install malicious tools by spawning a shell.\nOpen a NEW terminal window or tab. Don’t close the one tailing the Falco logs.\nFind one of your running application pods:\nkubectl get pods -l app=workshop-app Copy the full name of one of the pods (e.g., workshop-app-5f4b6c8b9d-abcde).\n“Shell” into the running container. This exec command gives you an interactive shell inside the container.\nkubectl exec -it \u003cyour-app-pod-name\u003e -- /bin/sh Your terminal prompt will change, indicating you are now inside the container (e.g., $ or #).\nWitness the Real-Time Detection Immediately switch back to your first terminal window (the one with the Falco logs).\nWithin seconds of executing the exec command, you will see a new JSON-formatted log entry from Falco. It will look similar to this:\n08:29:42.827362021: Notice A shell was spawned in a container with an attached terminal | evt_type=execve user=appuser user_uid=1001 user_loginuid=-1 process=sh proc_exepath=/usr/bin/dash parent=containerd-shim command=sh terminal=34816 exe_flags=EXE_LOWER_LAYER container_id=c2d197da82de container_name=workshop-app container_image_repository=593793056386.dkr.ecr.us-east-2.amazonaws.com/workshop-app container_image_tag=9fb43f1fb58cae94f85f5a8ba31c105b43b26068 k8s_pod_name=workshop-app-77d986f5b6-76tvd k8s_ns_name=default Info “Notice”: This is the default severity level for this rule.\nNow, let’s trigger a higher-severity alert.\nGo back to the terminal where you are inside the container.\nExit the container.\nexit Temporarily delete Kyverno require-non-root-user policy.\nkubectl delete clusterpolicy require-non-root-user Let’s create a nginx deployment:\nkubectl create deployment nginx --image=nginx Execute a command that would trigger a rule:\nkubectl exec -it $(kubectl get pods --selector=app=nginx -o name) -- cat /etc/shadow Switch back to the Falco logs. You will see another, more severe alert:\nYou will see logs for all the Falco pods deployed on the system. The Falco pod corresponding to the node in which our nginx deployment is running has detected the event, and you’ll be able to read a line like:\n08:58:14.478370676: Warning Sensitive file opened for reading by non-trusted program | file=/etc/shadow gparent=systemd ggparent=\u003cNA\u003e gggparent=\u003cNA\u003e evt_type=openat user=root user_uid=0 user_loginuid=-1 process=cat proc_exepath=/usr/bin/cat parent=containerd-shim command=cat /etc/shadow terminal=34816 container_id=4c908449279e container_name=nginx container_image_repository=docker.io/library/nginx container_image_tag=latest k8s_pod_name=nginx-5869d7778c-kfdjf k8s_ns_name=default Clean up and roll back: You can stop tailing the Falco logs by pressing Ctrl+C in that window. You should also re-apply Kyverno require-non-root-user policy.\nkubectl delete deployment nginx kubectl apply -f k8s/policy-require-non-root.yaml",
    "description": "We will focus first on the runtime security piece. Our mission is to detect and alert on suspicious activity within our running containers in real-time.\nOur tool for this is Falco, the CNCF open-source standard for cloud-native runtime threat detection. It acts like a security camera that watches system calls and alerts you when a container does something it shouldn’t.\nInstall Falco using Helm Just like with Kyverno, Helm is the most straightforward way to deploy Falco and its components.",
    "tags": [],
    "title": "Install and Test Falco for Runtime Threat Detection",
    "uri": "/install-and-test-falco-for-runtime-threat-detection/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "Overview After completing the workshop, or if you encounter any deployment failures, you should clean up your AWS resources to prevent unnecessary charges. This guide provides step-by-step instructions for properly removing all deployed resources.\nCleanup Process Follow these steps in the specified order to ensure all resources are properly deleted:\nStep 1: Delete EKS Cluster In your secure-container-pipeline project directory, run the following eksctl command:\neksctl delete cluster -f k8s/cluster.yaml --wait --disable-nodegroup-eviction --force --parallel 4 Step 2: Delete ECR Repository Run the AWS CLI command in your terminal:\naws ecr delete-repository \\ --repository-name workshop-app \\ --region us-east-2 # Use the same region as your cluster Step 3: Delete IAM Role and Identity provider In the AWS Console, go to IAM → Roles. Select WorkshopGitHubActionsRole role. Click Delete. Enter the role name to confirm deletion. Click Delete. Go to IAM → Roles Select token.actions.githubusercontent.com provider. Click Delete. Type confirm to confirm removal. Click Delete.",
    "description": "Overview After completing the workshop, or if you encounter any deployment failures, you should clean up your AWS resources to prevent unnecessary charges. This guide provides step-by-step instructions for properly removing all deployed resources.\nCleanup Process Follow these steps in the specified order to ensure all resources are properly deleted:\nStep 1: Delete EKS Cluster In your secure-container-pipeline project directory, run the following eksctl command:\neksctl delete cluster -f k8s/cluster.yaml --wait --disable-nodegroup-eviction --force --parallel 4 Step 2: Delete ECR Repository Run the AWS CLI command in your terminal:",
    "tags": [],
    "title": "Clean up resources",
    "uri": "/clean-up-resources/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Container Registry Security with Vulnerability Scanning and Policy Enforcement",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
